{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chain of Letters\n",
    "\n",
    "Only consider letters 'a-z'. Capital letters 'A-Z' are mapped to the corresponding ones. All remaining letters, symbols, numbers, including spaces, are denoted by '.'.\n",
    "\n",
    "\n",
    "We have a probability table $T$ where $T_{i,j} = p(x_t = j | x_{t-1} = i)$  transition model of letters in English text for $t=1,2 \\dots N$. Assume that the initial letter in a string is always a space denoted as $x_0 = \\text{'.'}$. Such a model where the probability table is always the same is sometimes called a stationary model.\n",
    "\n",
    "1. For a given $N$, write a program to sample random strings with letters $x_1, x_2, \\dots, x_N$ from $p(x_{1:N}|x_0)$\n",
    "1. Now suppose you are given strings with missing letters, where each missing letter is denoted by a question mark (or underscore, as below). Implement a method, that samples missing letters conditioned on observed ones, i.e., samples from $p(x_{-\\alpha}|x_{\\alpha})$ where $\\alpha$ denotes indices of observed letters. For example, if the input is 't??.', we have $N=4$ and\n",
    "$x_1 = \\text{'t'}$ and $x_4 = \\text{'.'}$, $\\alpha=\\{1,4\\}$ and $-\\alpha=\\{2,3\\}$. Your program may possibly generate the strings 'the.', 'twi.', 'tee.', etc. Hint: make sure to make use all data given and sample from the correct distribution. Implement the method and print the results for the test strings below. \n",
    "1. Describe a method for filling in the gaps by estimating the most likely letter for each position. Hint: you need to compute\n",
    "$$\n",
    "x_{-\\alpha}^* = \\arg\\max_{x_{-\\alpha}} p(x_{-\\alpha}|x_{\\alpha})\n",
    "$$\n",
    "Implement the method and print the results for the following test strings along with the log-probability  $\\log p(x_{-\\alpha}^*,x_{\\alpha})$.\n",
    "1. Discuss how you can improve the model to get better estimations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_strings = ['th__br__n.f_x.', '_u_st__n_.to_be._nsw_r__','i__at_._a_h_n_._e_r_i_g','q___t.___z._____t.__.___.__.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$p(x_t = \\text{'u'} | x_{t-1} = \\text{'q'})$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9949749\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$p(x_t | x_{t-1} = \\text{'a'})$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', '0.0002835')\n",
      "('b', '0.0228302')\n",
      "('c', '0.0369041')\n",
      "('d', '0.0426290')\n",
      "('e', '0.0012216')\n",
      "('f', '0.0075739')\n",
      "('g', '0.0171385')\n",
      "('h', '0.0014659')\n",
      "('i', '0.0372661')\n",
      "('j', '0.0002353')\n",
      "('k', '0.0110124')\n",
      "('l', '0.0778259')\n",
      "('m', '0.0260757')\n",
      "('n', '0.2145354')\n",
      "('o', '0.0005459')\n",
      "('p', '0.0195213')\n",
      "('q', '0.0001749')\n",
      "('r', '0.1104770')\n",
      "('s', '0.0934290')\n",
      "('t', '0.1317960')\n",
      "('u', '0.0098029')\n",
      "('v', '0.0306574')\n",
      "('w', '0.0088799')\n",
      "('x', '0.0009562')\n",
      "('y', '0.0233701')\n",
      "('z', '0.0018701')\n",
      "('.', '0.0715219')\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from IPython.display import display, Latex\n",
    "\n",
    "alphabet = [chr(i+ord('a')) for i in range(26)]\n",
    "alphabet.append('.')\n",
    "letter2idx = {c:i for i,c in enumerate(alphabet)}\n",
    "\n",
    "T = []\n",
    "with open('transitions.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        T.append(row)\n",
    "\n",
    "print('Example')\n",
    "## p(x_t = 'u' | x_{t-1} = 'q')\n",
    "display(Latex(r\"$p(x_t = \\text{'u'} | x_{t-1} = \\text{'q'})$\"))\n",
    "print(T[letter2idx['q']][letter2idx['u']])\n",
    "display(Latex(r\"$p(x_t | x_{t-1} = \\text{'a'})$\"))\n",
    "for c,p in zip(alphabet,T[letter2idx['a']]):\n",
    "    print(c,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1. Sampling Random Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "Sample String Given $S_0$ only"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wpneeeef.edun..t.aasixoywm..roewsdoehhhhs.ttdeemmgo.k.toncerhmboemhospaa.osct.fsw.fultcu..sstl.e.drore...tp.mshreoet.y..d.lbhenhsh..n.guet.e..owao.acsrrm..out..hrdpoirenlnaowei.o..e..uoiae..cf..d.lsysleloehteita..ieiotoa.aiidiidadihhpo.niaarz.tesncdsn.inlfaeet.o..olrpiesb..i.eolasieelk.ng.e.eymlnh.nrrae.tkaandect..adrm.y.smaegtthon.mg.h.fkolnhetirh.so.o.ne.oaevd.otnosu.ait.g.oongdrltgtry.hetsai.si.hi.ao.fyhiat.ytdaeto.oceaa.eenhoy.i.sm.es.anernge.ennrhlt.owaioteiottfietrfoedee.aetritod....wrafe..so.onhile.stt.rctqshgs....uadaomsens..yi.eeahui.o.e.ia.oecin.yv..erbanreu.no.v.ee.ifcodaa..labf.l..s.v..teowettghhlsnltnbk..io.t.f.hntmso.d.l.i.hraee.uihai..sn.oei.ea...qt..t..eitronof.cg..afhshencuss.vontnshbilaebtgs.annhs..houfn.luehco.tsfttet.idtsn.o.sefairt.horoh.cua.elifenod.oi.edl.nviaossi.no.ltm.nnufweihuaoheafraa.lo.uea.ettdefoettaygahhdsalnrrso.i.tsiioaneussramhs.s.faw.aarhelt...eo..y.hntshweceirrawimc.oehos.q.aaememn.nnscwto..or.lroaabnicthesoces.vu.ff.leli.eilnueaaa.t.sbfhbim.e.sstta\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "Sample String Given $S_{i-1}$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lly.theeaveng.thang.an.cht.med.thengale.warduprameni.sosco.t..pity.swotonve.t.oublall.apry.at.dongiche.we.st..owon.se.mitimimporeng.os.f.horth.arowhed.wri.matero.ver.gusm.hior..ste.my.whe.st.th.onathan.n.bin.abss.tin.hu.oth.sad.imalsiosthe.as.aros..ti.mallt.osd.helld.my.t..lopheen.at.ant.d.mag.finunthesher.in..ithe.d.t.sse.tomad.s.prepeheanethenfffoutitis.re.outhr.tathara.athe.wainso.veoreatly.h.san.totof.je.thran.song.irrownst..aso.sontu.bucorrrsoi.rditrandsthauatherio.llion.c.y.blis.the.ow.chemoreod.trlestevy.an.fenutwh.ntiant.inger.ber.ftorad.onedosioh.t.w.s.uregiatigrelave.thallly.ftus.bas.fushe..s.tovepp.he.githautis.te.tsthe.ethy.athe.licth.ventiselindesen.cad...ingeid.bemid.ime.ondit.tend.bjaindom.masheatht.tomonivey.t.claneyon..ove.itiorat.wiote.pr.wamintoun.fe.whon.ppsul.opls.arge.pelm.cth.o.hicon.wat.sumeeme.sar.njur.imy.at.sseate.mamandernir.ar.c.tungeerk.t.f.and.ls.sappe.cemoustad.th.ouroorntarenth..se.t.ang.hin.umirred.tat.f.hamund.ainthancat.rz.come.theey.abledrth.s.hage.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "#Sum of the probabilites for some columns are sometimes slightyly more than 1.(ex. 1.0000523)\n",
    "#Normalize function normalizes the probabilities to exactly 1\n",
    "def Normalize(K):\n",
    "    for i in range(len(K)):\n",
    "        temp=map(float,K[i])\n",
    "        K[i] = [float(j)/sum(temp) for j in temp]\n",
    "    return K\n",
    "\n",
    "#Samples from the distribution applying Markov(1) and returns a sequence with given number of letters\n",
    "#sampleString generates letters one by one and takes the last one as given in order to generate the next one\n",
    "\n",
    "def sampleString(N):\n",
    "    s='.'\n",
    "    seq=''\n",
    "    K=Normalize(T)\n",
    "    for n in range(0,N):\n",
    "        probs=K[letter2idx[s]]\n",
    "        s = alphabet[np.random.choice(np.arange(0, 27), p=probs)]\n",
    "        seq=seq+s\n",
    "    return seq\n",
    "\n",
    "#Samples from the distribution applying Markov(1) and returns a sequence with given number of letters\n",
    "#sampleString generates letters takes only the '.' as given and generate each sequence by multiplying the transition probabilities\n",
    "def sampleStringFromInitial(N):\n",
    "    s='.'\n",
    "    seq=''\n",
    "    K=Normalize(T)\n",
    "    TransitionM = np.vstack(K).astype(float)\n",
    "    for n in range(0,N):\n",
    "        seq = seq + alphabet[np.random.choice(np.arange(0, 27), p=TransitionM[letter2idx[s]])]\n",
    "        TransitionM = np.dot(TransitionM,K)\n",
    "    return seq\n",
    "\n",
    "display(Latex(r'Sample String Given $S_0$ only')) \n",
    "print sampleStringFromInitial(1000)\n",
    "\n",
    "\n",
    "display(Latex(r'Sample String Given $S_{i-1}$')) \n",
    "print sampleString(1000)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2. Fill the Missing Letters by Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define all letter sequences as state $S_t$. \n",
    "Since we use Markov(1) model we only need to look at the closest given letters from both sides to fill the missing letters. \n",
    "\n",
    "Assume that $S_i$ is not given and we want to predict $S_i$.\n",
    "$S_l$ is the closest given letter to $S_i$ from left side and $S_r$ is the closest given letter from the right side.\n",
    "\n",
    "Then:\n",
    "\n",
    "If both $S_l$ and $S_r$ exist:\n",
    "\n",
    "$P\\left(S_i\\ |\\ S_l=l,S_r=r\\right) = \\frac{P\\left(S_i,S_l=l,S_r=r\\right)}{P\\left(S_l=l,S_r=r\\right)}$\n",
    "\n",
    "$= \\frac{P\\left(S_l=l\\right)*P\\left(S_i\\ |\\ S_l=l\\right)*P\\left(S_r=r\\ |\\ S_i\\right)}{P\\left(S_r=r\\ |\\ S_l=l\\right)*P\\left(S_l=l\\right)}$\n",
    "\n",
    "$= \\frac{P\\left(S_i\\ |\\ S_l=l\\right)*P\\left(S_r=r\\ |\\ S_i\\right)}{P\\left(S_r=r\\ |\\ S_l=l\\right)}$\n",
    "\n",
    "If only $S_l$ exists:\n",
    "\n",
    "$P\\left(S_i\\ |\\ S_l=l\\right) = \\frac{P\\left(S_i,S_l=l\\right)}{P\\left(S_l=l\\right)}$\n",
    "\n",
    "$= \\frac{P\\left(S_l=l\\right)*P\\left(S_i\\ |\\ S_l=l\\right)}{P\\left(S_l=l\\right)}$\n",
    "\n",
    "$=P\\left(S_i\\ |\\ S_l=l\\right)$\n",
    "\n",
    "Since Markov(1) model is used ,conditional probabilities represented above are calculated by taking the powers of the transition matrix depending on the distance between the indexes of given letters and the indexes of the empty letters.\n",
    "\n",
    "In this part, after calculating the desired probability, sampling is done from the obtained probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.Code & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text  1   Sample  1  :  ths.bry.n.fex.\n",
      "Text  1   Sample  2  :  the.brven.fix.\n",
      "Text  1   Sample  3  :  the.br.in.fex.\n",
      "Text  1   Sample  4  :  the.brern.fex.\n",
      "Text  1   Sample  5  :  the.br.an.fex.\n",
      "Text  1   Sample  6  :  the.br.un.fex.\n",
      "Text  1   Sample  7  :  thr.br.an.fex.\n",
      "Text  1   Sample  8  :  tha.br.in.fex.\n",
      "Text  1   Sample  9  :  th.abryan.fex.\n",
      "Text  1   Sample  10  :  tha.broun.fex.\n",
      "Text  1   Sample  11  :  theibrd.n.fex.\n",
      "Text  1   Sample  12  :  th.abrean.fox.\n",
      "Text  1   Sample  13  :  th.bbrf.n.fex.\n",
      "Text  1   Sample  14  :  the.br.in.fex.\n",
      "Text  1   Sample  15  :  the.bre.n.fex.\n",
      "Text  1   Sample  16  :  the.br.in.fex.\n",
      "Text  1   Sample  17  :  thoubrs.n.fex.\n",
      "Text  1   Sample  18  :  th.obrcon.fex.\n",
      "Text  1   Sample  19  :  the.brean.fex.\n",
      "Text  1   Sample  20  :  the.br.in.f.x.\n",
      "Text  2   Sample  1  :  oursthino.toube.answorvi\n",
      "Text  2   Sample  2  :  tusstheny.to.be.inswor.a\n",
      "Text  2   Sample  3  :  buesthany.to.be.inswir.a\n",
      "Text  2   Sample  4  :  oulsthiny.toabe.answartt\n",
      "Text  2   Sample  5  :  buisthany.toobe.answar.i\n",
      "Text  2   Sample  6  :  sursthind.toube.onswer.r\n",
      "Text  2   Sample  7  :  gu.sthand.toube.inswaril\n",
      "Text  2   Sample  8  :  mursthine.toube.ensw.rro\n",
      "Text  2   Sample  9  :  fumsthend.to.be.inswaro.\n",
      "Text  2   Sample  10  :  russthins.to.be.answorel\n",
      "Text  2   Sample  11  :  ougst.hnd.to.be.answir.t\n",
      "Text  2   Sample  12  :  auisthind.toobe.onswar.s\n",
      "Text  2   Sample  13  :  ouisthand.to.be.inswir.w\n",
      "Text  2   Sample  14  :  ounstheng.to.be.inswared\n",
      "Text  2   Sample  15  :  julstound.tombe.inswarep\n",
      "Text  2   Sample  16  :  qutsthonk.to.be.answ.re.\n",
      "Text  2   Sample  17  :  oursthind.toube.answer.l\n",
      "Text  2   Sample  18  :  pussttina.to.be.inswere.\n",
      "Text  2   Sample  19  :  burstomnd.to.be.insworta\n",
      "Text  2   Sample  20  :  buisthond.tombe.answared\n",
      "Text  3   Sample  1  :  im.ate.wathang.ce.r.ing\n",
      "Text  3   Sample  2  :  it.at..hachino.feersing\n",
      "Text  3   Sample  3  :  iveath.iathene.beerting\n",
      "Text  3   Sample  4  :  ie.atr.rachont.peprdirg\n",
      "Text  3   Sample  5  :  ingato..athind.he.r.ing\n",
      "Text  3   Sample  6  :  ithatr.wa.hend.le.rring\n",
      "Text  3   Sample  7  :  ia.aty.machint.herrding\n",
      "Text  3   Sample  8  :  inoath..athend.se.rsing\n",
      "Text  3   Sample  9  :  ierats.tashane.be.r.ing\n",
      "Text  3   Sample  10  :  intaty.pa.hend.lear.ing\n",
      "Text  3   Sample  11  :  i.tath.sawhen..heor.ing\n",
      "Text  3   Sample  12  :  is.ate.tashun..pearging\n",
      "Text  3   Sample  13  :  if.ate.nathins.ye.roing\n",
      "Text  3   Sample  14  :  is.at..mashend.we.rsi.g\n",
      "Text  3   Sample  15  :  is.atw.wathine.he.r.idg\n",
      "Text  3   Sample  16  :  i.mats.pa.heng.reerviug\n",
      "Text  3   Sample  17  :  ighato.wathang.iedreing\n",
      "Text  3   Sample  18  :  iseath.pathend.heirving\n",
      "Text  3   Sample  19  :  iocatt.bashent.ee.r.ing\n",
      "Text  3   Sample  20  :  ithate.fathand.de.r.ing\n",
      "Text  4   Sample  1  :  qunet.s.iz.breyot..s.ore.t..\n",
      "Text  4   Sample  2  :  qud.t.berz.rmat.t.ss.ong.at.\n",
      "Text  4   Sample  3  :  qurut.thiz.momist.te.ind.th.\n",
      "Text  4   Sample  4  :  qudit.sorz.ngly.t.t..fis.od.\n",
      "Text  4   Sample  5  :  qut.t..ouz.he.tht.am.the.so.\n",
      "Text  4   Sample  6  :  qum.t.kuiz.herout.cr.ort.we.\n",
      "Text  4   Sample  7  :  qut.t.ourz.w.arit.by.son.is.\n",
      "Text  4   Sample  8  :  quait.berz.licent.ed.aie.ng.\n",
      "Text  4   Sample  9  :  qumit.serz.bune.t.me.hey.ay.\n",
      "Text  4   Sample  10  :  qutit.burz.jurhit.sh.wan..t.\n",
      "Text  4   Sample  11  :  qusst.atoz.f.an.t.th.get.ll.\n",
      "Text  4   Sample  12  :  qunst.whiz.theext.ss.itr.nd.\n",
      "Text  4   Sample  13  :  qu.it.ctoz.a.s.ft.de.owe.bu.\n",
      "Text  4   Sample  14  :  qum.t.harz.by.m.t.ty.i.l.mr.\n",
      "Text  4   Sample  15  :  qusit.s.iz..ced.t..i.wnd.mu.\n",
      "Text  4   Sample  16  :  qucat.thaz.hind.t.he.grs.be.\n",
      "Text  4   Sample  17  :  qugst.ourz.l.id.t.le.m.e..t.\n",
      "Text  4   Sample  18  :  quiot.t.iz.sere.t.or.hts..w.\n",
      "Text  4   Sample  19  :  qul.t.carz.isthat.my.ing.ld.\n",
      "Text  4   Sample  20  :  qur.t.wauz.gin.st.pl.bun.he.\n"
     ]
    }
   ],
   "source": [
    "def findUnknownIndex(st):\n",
    "    unknown=list()\n",
    "    known = list()\n",
    "    for i in range(0,len(st)):\n",
    "        if st[i] is '_' or st[i] is '?' :\n",
    "            unknown.append(i)\n",
    "        else:\n",
    "            known.append(i)\n",
    "    return unknown,known\n",
    "def findNeighbors(k,st):\n",
    "    i=1\n",
    "    sn=None\n",
    "    bn=None\n",
    "    while st[k-i]:\n",
    "        if st[k-i] is not '_' and st[i] is not '?':\n",
    "            sn=k-i\n",
    "            break\n",
    "        i=i+1\n",
    "    i=1\n",
    "    try:\n",
    "        while st[k+i]:\n",
    "            if st[k+i] is not '_' and st[i] is not '?':\n",
    "                bn=k+i\n",
    "                break\n",
    "            i=i+1\n",
    "    except:\n",
    "        bn = None\n",
    "    return sn,bn\n",
    "def predictMissingLetters(st,sample=False):\n",
    "    probMatrix=Normalize(T)\n",
    "    st = '.'+st\n",
    "    unknownIndexes,knownIndexes = findUnknownIndex(st)\n",
    "    p = list()\n",
    "    missingValues=dict()\n",
    "    for k in unknownIndexes:\n",
    "        sn,bn=findNeighbors(k,st)\n",
    "        \n",
    "        if sn is not None and bn is not None:\n",
    "            forward = np.vstack(probMatrix).astype(float)\n",
    "            backward = np.vstack(probMatrix).astype(float)\n",
    "            normalizer = np.vstack(probMatrix).astype(float)\n",
    "            for i in range(1,k-sn):\n",
    "                forward=np.dot(forward,np.vstack(probMatrix).astype(float))\n",
    "            for i in range(1,bn-k):\n",
    "                backward=np.dot(backward,np.vstack(probMatrix).astype(float))\n",
    "            for i in range(1,bn-sn):\n",
    "                normalizer=np.dot(normalizer,np.vstack(probMatrix).astype(float))\n",
    "            bm = backward[:,letter2idx[st[bn]]]\n",
    "            fm = forward[letter2idx[st[sn]],:]\n",
    "            nm = normalizer[letter2idx[st[sn]]][letter2idx[st[bn]]]\n",
    "            predictionProb = np.multiply(bm,fm)/nm\n",
    "            if sample:\n",
    "                st=list(st)\n",
    "                st[k]=alphabet[np.random.choice(np.arange(0, 27), p=predictionProb)]\n",
    "                st=\"\".join(st)\n",
    "            else:\n",
    "                st=list(st)\n",
    "                st[k]=alphabet[predictionProb.tolist().index(max(predictionProb))]\n",
    "                p.append(max(predictionProb))\n",
    "                st=\"\".join(st)\n",
    "        else:\n",
    "            forward = np.vstack(probMatrix).astype(float)\n",
    "            for i in range(1,k-sn):\n",
    "                forward=np.dot(forward,np.vstack(probMatrix).astype(float))\n",
    "            fm = forward[letter2idx[st[sn]],:]\n",
    "            predictionProb = fm\n",
    "            if sample:\n",
    "                st=list(st)\n",
    "                st[k]=alphabet[np.random.choice(np.arange(0, 27), p=predictionProb)]\n",
    "                st=\"\".join(st)\n",
    "            else:\n",
    "                st=list(st)\n",
    "                st[k]=alphabet[predictionProb.tolist().index(max(predictionProb))]\n",
    "                p.append(max(predictionProb))\n",
    "                st=\"\".join(st)\n",
    "    total = 1\n",
    "    for x in p:\n",
    "        total *= x   \n",
    "    return st[1:len(st)],math.log(total)\n",
    "\n",
    "def sampleMissingLetters(st,Ntrial):\n",
    "    for k in st:\n",
    "        for trial in range(0,Ntrial):\n",
    "            print 'Text ',st.index(k)+1,\" \",'Sample ',trial+1,\" : \",predictMissingLetters(k,sample=True)[0]\n",
    "            \n",
    "def predictMostLikelyMissingLetters(st):\n",
    "    for k in st:\n",
    "        txt,probs=predictMissingLetters(k,sample=False)\n",
    "        print 'String: ',txt\n",
    "        print 'Log-Probability: '\n",
    "        print probs\n",
    "sampleMissingLetters(test_strings,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3. Fill the Missing Letters By Most Likely Letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String:  t.ll\n",
      "Log-Probability: \n",
      "-1.38208334111\n"
     ]
    }
   ],
   "source": [
    "predictMostLikelyMissingLetters(test_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. How to Improve the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this model we are using Markov(1) model, this means that we are considering only the closest given letters from both sides. Although it is a very simple approach and efficient to calculate, a letter in a word is also related to the other words in a model. Sequencing of letters in a word is very important feature that should be considered. Therefore using higher order Markov model possibly give better results.\n",
    "Also punctuations and uppercase letters should be implemented in the model. Transition probabilities for punctuations should be added since they also carry significant information in terms of predicting the previous and the next letter. Uppercase letters carry some information too. Begining words of the sentences would change the transition matrix and make it more useful.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
